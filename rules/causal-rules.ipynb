{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The notebook contains the regex rules to identify causal questions and the code to parse datasets\n",
    "### You should download original datasets from the respective sources (links are provided in the starting README file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 rules to identify causal questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "pattern1 = re.compile(r\"\\Awhy|(?=\\Aif)(?=.*why )|(?=\\Awhen)(?=.*why )| and why | why is \\w+ | is \\w+ why \", re.IGNORECASE)\n",
    "pattern2 = re.compile(r\"\\Acause.{0,1} | cause.{0,1} |because of what\", re.IGNORECASE)\n",
    "pattern3 = re.compile(\"\\s*how come |\\s*how did \", re.IGNORECASE)\n",
    "pattern4 = re.compile(r\"^(?!.*dopplar).*effect.{0,1} .*$| affect{0,1} \", re.IGNORECASE)\n",
    "pattern5 = re.compile(' lead to', re.IGNORECASE)\n",
    "pattern6 = re.compile(r\"(?=.*what happens)(?=.*if)|(?=.*what will happen)(?=.*if)|(?=.*what might happen)(?=.*if)|\\\n",
    "(?=.*what happens)(?=.*when)|(?=.*what will happen)(?=.*when)|(?=.*what might happen)(?=.*when)\", re.IGNORECASE)\n",
    "pattern7 = re.compile(\"\\Awhat to do if |\\Awhat to do when |\\Awhat to do to |\\\n",
    "\\Awhat should be done if |\\Awhat should be done when |\\Awhat should be done to \", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punct(s):\n",
    "    '''Removes all characters except Russian and Latin letters'''\n",
    "    s = re.sub('[^А-Яа-яЁёЙйA-Za-z0-9]', ' ', s.lower())\n",
    "    return \" \".join(s.lower().split())\n",
    "\n",
    "def identify_causal(json_file, question_field):\n",
    "    causal_entries = list()   \n",
    "    for jline in json_file:\n",
    "        question = strip_punct(json.loads(jline)[question_field])\n",
    "        if pattern1.search(question) or pattern2.search(question) or pattern3.search(question)\\\n",
    "        or pattern4.search(question) or pattern5.search(question) or pattern6.search(question) or pattern7.search(question):\n",
    "            causal_entries.append(json.loads(jline))        \n",
    "    return causal_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_field = 'question' # PAQ, GooAQ, NewsQA\n",
    "# question_field = 'question_text' # Natural Questions\n",
    "\n",
    "\n",
    "with open('path_to_json_data_file.jsonl', 'r') as jfile:\n",
    "    causal_entries = identify_causal(jfile, question_field)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HotpotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def hotpot_identify_causal(dicts):\n",
    "    causal_entries = list()\n",
    "    for d in dicts:\n",
    "        question = d['question']\n",
    "        qs.append(question)\n",
    "        q = strip_punct(question)\n",
    "        if pattern1.search(q) or pattern2.search(q) or pattern3.search(q)\\\n",
    "        or pattern4.search(q) or pattern5.search(q) or pattern6.search(q) or pattern7.search(q):\n",
    "            causal_entries.append(d)\n",
    "    return causal_entries\n",
    "\n",
    "PATH = '' # path to the hotpot-master directory\n",
    "\n",
    "train_filename = 'hotpot_train_v1.1.json'\n",
    "\n",
    "with open(os.path.join(PATH, train_filename), 'r') as f:\n",
    "    train_dicts = json.load(f)\n",
    "    \n",
    "dev_filename = 'hotpot_dev_distractor_v1.json'\n",
    "\n",
    "with open(os.path.join(PATH, dev_filename), 'r') as f:\n",
    "    test_dicts = json.load(f)\n",
    "            \n",
    "        \n",
    "train_causal_entries, test_causal_entries = hotpot_identify_causal(train_dicts), hotpot_identify_causal(test_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MS MARCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "def msmarco_identify_causal(question):\n",
    "    q = strip_punct(question)\n",
    "    if pattern1.search(q) or pattern2.search(q) or pattern3.search(q)\\\n",
    "    or pattern4.search(q) or pattern5.search(q) or pattern6.search(q) or pattern7.search(q): \n",
    "        return True\n",
    "\n",
    "PATH = '' # path to the MS MARCO directory    \n",
    "        \n",
    "with gzip.GzipFile(os.path.join(PATH,'train_v2.1.json.gz'), 'r') as f:\n",
    "    data_train = json.loads(f.read().decode('utf-8'))\n",
    "with gzip.GzipFile(os.path.join(PATH,'dev_v2.1.json.gz'), 'r') as f:\n",
    "    data_dev = json.loads(f.read().decode('utf-8'))\n",
    "\n",
    "causal_entries = list()\n",
    "\n",
    "dataset = data_train\n",
    "# dataset = data_dev # uncomment for the dev dataset\n",
    "\n",
    "for data in dataset:\n",
    "    for query, qid, wellFormedAnswers, passages, answers, query_type in zip(list(data['query'].values()), list(data['query_id'].values()), \n",
    "                                                                            list(data['wellFormedAnswers'].values()), list(data['passages'].values()), \n",
    "                                                                            list(data['answers'].values()), list(data['query_type'].values())):\n",
    "        if query msmarco_identify_causal(cause_questions):\n",
    "            d = {}\n",
    "            question = query\n",
    "            d['answers'] = answers\n",
    "            d['passages'] = passages\n",
    "            d['query'] = query\n",
    "            d['query_id'] = qid\n",
    "            d['query_type'] = query_type\n",
    "            d['wellFormedAnswers'] = wellFormedAnswers\n",
    "            causal_entries.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELI5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "\n",
    "def eli_identify_causal(data):\n",
    "    causal_entries = list()\n",
    "    for line in data:\n",
    "        question = jline['title']\n",
    "        qs.append(question)\n",
    "        q = strip_punct(question)\n",
    "        if pattern1.search(q) or pattern2.search(q) or pattern3.search(q)\\\n",
    "        or pattern4.search(q) or pattern5.search(q) or pattern6.search(q) or pattern7.search(q):\n",
    "            causal_entries.append(line)\n",
    "    return causal_entries\n",
    "\n",
    "eli5 = nlp.load_dataset('eli5')\n",
    "\n",
    "train_data = eli5['train_eli5']\n",
    "dev_data = eli5['validation_eli5']\n",
    "\n",
    "train_causal_entries = eli_identify_causal(train_data)\n",
    "dev_causal_entries = eli_identify_causal(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SearchQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "path = '/SearchQA/data_json/train.zip' # path to the train zip\n",
    "\n",
    "train_data = list()\n",
    "\n",
    "with zipfile.ZipFile(path, 'r') as z:\n",
    "    for filename in tqdm(z.namelist()):\n",
    "        with z.open(filename, 'r') as f:\n",
    "            for d in f:\n",
    "                train_data.append(d)\n",
    "                \n",
    "path = 'SearchQA/data_json/val.zip' # path to the val zip\n",
    "\n",
    "val_data = list()\n",
    "\n",
    "with zipfile.ZipFile(path, 'r') as z:\n",
    "    for filename in tqdm(z.namelist()):\n",
    "        with z.open(filename, 'r') as f:\n",
    "            for d in f:\n",
    "                val_data.append(d)\n",
    "                \n",
    "train_causal_entries = identify_causal(train_data, 'question')\n",
    "dev_causal_entries = identify_causal(val_data, 'question')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQuaD 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def squad_identify_causal(questions):\n",
    "    causal_questions = list()\n",
    "    for question in questions:\n",
    "        q = strip_punct(question)\n",
    "        if pattern1.search(q) or pattern2.search(q) or pattern3.search(q)\\\n",
    "        or pattern4.search(q) or pattern5.search(q) or pattern6.search(q) or pattern7.search(q):\n",
    "            causal_questions.append(question)\n",
    "    return causal_questions\n",
    "\n",
    "PATH = '' # path to the SQuaD directory\n",
    "\n",
    "with open(os.path.join(PATH,'train-v2.0.json'), 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(os.path.join(PATH,'dev-v2.0.json'), 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "    \n",
    "squad_questions = list()\n",
    "\n",
    "for data in [train_data, dev_data]:\n",
    "    for i1 in data['data']:\n",
    "        data_out.append(i1)\n",
    "        for i2 in i1['paragraphs']:\n",
    "            for i3 in i2['qas']:\n",
    "                squad_questions.append(i3['question'])\n",
    "                \n",
    "\n",
    "squad_causal_questions = squad_identify_causal(squad_questions)\n",
    "\n",
    "dicts_train = list()\n",
    "for entry in train_data['data']:\n",
    "    for entry1 in entry['paragraphs']:\n",
    "        for entry2 in entry1['qas']:\n",
    "            if entry2['question'] in squad_causal_questions:\n",
    "                entry2['context'] = entry1['context']\n",
    "                entry2['title'] = entry['title']\n",
    "                dicts_train.append(entry2)\n",
    "                \n",
    "dicts_dev = list()\n",
    "for entry in dev_data['data']:\n",
    "    for entry1 in entry['paragraphs']:\n",
    "        for entry2 in entry1['qas']:\n",
    "            if entry2['question'] in squad_causal_questions:\n",
    "                entry2['context'] = entry1['context']\n",
    "                entry2['title'] = entry['title']\n",
    "                dicts_dev.append(entry2)\n",
    "                \n",
    "train_cuasal_entries = dicts_train\n",
    "dev_causal_entries = dicts_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NewsQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_newsqa = pd.read_csv('combined-newsqa-data-v1.csv')\n",
    "questions_newsqa = df_newsqa.question.tolist()\n",
    "\n",
    "def newsqa_identify_causal(questions):\n",
    "    causal_questions = list()\n",
    "    \n",
    "    for question in questions:\n",
    "        if type(question) == str:\n",
    "            qs.append(question)\n",
    "            q = strip_punct(question)\n",
    "            if pattern1.search(q) or pattern2.search(q) or pattern3.search(q)\\\n",
    "            or pattern4.search(q) or pattern5.search(q) or pattern6.search(q) or pattern7.search(q):\n",
    "                causal_questions.append(question)\n",
    "    return causal_questions\n",
    "                \n",
    "causal_questions = newsqa_identify_causal(questions_newsqa)\n",
    "\n",
    "df_res = df_newsqa.loc[df_newsqa['question'].isin(causal_questions)]\n",
    "df_res_json = df_res.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "PATH = '' # path to the TriviaQA directory\n",
    "\n",
    "with open(os.path.join(PATH, 'unfiltered-web-train.json'), 'r') as z:\n",
    "    data = z.read()\n",
    "json_train = json.loads(data)\n",
    "\n",
    "with open(os.path.join(PATH, 'unfiltered-web-dev.json'), 'r') as z:\n",
    "    data = z.read()\n",
    "json_dev = json.loads(data)\n",
    "\n",
    "def trivia_identify_causal(json_data):\n",
    "    causal_entries = list()\n",
    "    for jline in json_data['Data']:\n",
    "        question = jline['Question']\n",
    "        q = strip_punct(question)\n",
    "        if pattern1.search(q) or pattern2.search(q) or pattern3.search(q)\\\n",
    "        or pattern4.search(q) or pattern5.search(q) or pattern6.search(q) or pattern7.search(q):\n",
    "            causal_entries.append(jline)\n",
    "    return causal_entries\n",
    "\n",
    "\n",
    "train_cuasal_entries = trivia_identify_causal(json_train)\n",
    "dev_causal_entries = trivia_identify_causal(json_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
